{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(size=(32,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data',train=True,download=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data', train=False,download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1000,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex = nn.Sequential(nn.Conv2d(3,64,3,1,1),\n",
    "                                  nn.Conv2d(64,64,3,1,1),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(64,128,3,1,1),\n",
    "                                  nn.Conv2d(128,128,3,1,1),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(128,256,3,1,1),\n",
    "                                  nn.Conv2d(256,256,3,1,1),\n",
    "                                  nn.Conv2d(256,256,1,1),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(256,512,3,1,1),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.Conv2d(512,512,1,1),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.Conv2d(512,512,1,1),\n",
    "                                  nn.Flatten())(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alexnet(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.alex = nn.Sequential(nn.Conv2d(3,64,3,1,1),\n",
    "                                  nn.BatchNorm2d(64,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(64,64,3,1,1),\n",
    "                                  nn.BatchNorm2d(64,eps=0.001),nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(64,128,3,1,1),\n",
    "                                  nn.BatchNorm2d(128,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(128,128,3,1,1),\n",
    "                                  nn.BatchNorm2d(128,eps=0.001),nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(128,256,3,1,1),\n",
    "                                  nn.BatchNorm2d(256,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(256,256,3,1,1),\n",
    "                                  nn.BatchNorm2d(256,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(256,256,1,1),\n",
    "                                  nn.BatchNorm2d(256,eps=0.001),nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(256,512,3,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(512,512,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.MaxPool2d(3,2,1),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(512,512,3,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Conv2d(512,512,1,1),\n",
    "                                  nn.BatchNorm2d(512,eps=0.001),nn.ReLU(),\n",
    "                                  nn.Flatten(),\n",
    "                                  nn.Linear(2048,4096),nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.5),\n",
    "                                  nn.Linear(4096,4096),nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.5),\n",
    "                                  nn.Linear(4096,1000),nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        x = self.alex(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model\n"
     ]
    }
   ],
   "source": [
    "model = Alexnet().cuda()\n",
    "try:\n",
    "    model.load_state_dict(torch.load('C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\Neural Networks\\\\Pytorch implementation\\\\All Models Saved\\\\AlexNet.pth'))\n",
    "except:\n",
    "    print(\"No saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3,momentum=0.5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer,\n",
    "                                                 base_lr=1e-4,\n",
    "                                                 max_lr=1e-3,\n",
    "                                                 step_size_up=5,\n",
    "                                                 step_size_down=5,\n",
    "                                                 mode=\"exp_range\",\n",
    "                                                 cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i ,data in enumerate(test_loader,0):\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(),labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total+=labels.size(0)\n",
    "            accuracy+=(predicted == labels).sum().item()\n",
    "    \n",
    "    print(\"{}/{}\".format(accuracy,total))\n",
    "    accuracy = float(100 * accuracy / total)\n",
    "    \n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    for ep in range(1,epoch+1):\n",
    "        print(\"Epoch : \",ep)\n",
    "        running_loss=0\n",
    "        for i, data in enumerate(train_loader,0):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.cuda(),labels.cuda()\n",
    "            optimizer.zero_grad(set_to_none=False)\n",
    "            outputs = model(inputs)\n",
    "            #outputs_googLeNet = torch.add(torch.add(outputs[0]*0.4, outputs[1]*0.3).cuda(),outputs[2]*0.3).cuda()\n",
    "            loss = loss_function(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "            if (i+1)%1000 == 0 :\n",
    "                print(\"[{}/{}]\".format(i+1,12000))\n",
    "                print(\"Loss of the training model is : %.4f\"%(running_loss/1000))\n",
    "                running_loss=0\n",
    "        lr_scheduler.step()\n",
    "        accuracy = test()\n",
    "        print('For epoch', ep,'the test accuracy over the whole test set is %.2f %%' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.6344\n",
      "[2000/50000]\n",
      "Loss of the training model is : 4.5660\n",
      "[3000/50000]\n",
      "Loss of the training model is : 4.4955\n",
      "[4000/50000]\n",
      "Loss of the training model is : 4.4634\n",
      "[5000/50000]\n",
      "Loss of the training model is : 4.4058\n",
      "[6000/50000]\n",
      "Loss of the training model is : 4.3366\n",
      "[7000/50000]\n",
      "Loss of the training model is : 4.2971\n",
      "[8000/50000]\n",
      "Loss of the training model is : 4.2246\n",
      "[9000/50000]\n",
      "Loss of the training model is : 4.1793\n",
      "[10000/50000]\n",
      "Loss of the training model is : 4.1102\n",
      "[11000/50000]\n",
      "Loss of the training model is : 4.0971\n",
      "[12000/50000]\n",
      "Loss of the training model is : 4.0313\n",
      "907.0/10000.0\n",
      "For epoch 1 the test accuracy over the whole test set is 9.07 %\n",
      "Epoch :  2\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8773\n",
      "[2000/50000]\n",
      "Loss of the training model is : 3.8083\n",
      "[3000/50000]\n",
      "Loss of the training model is : 3.7032\n",
      "[4000/50000]\n",
      "Loss of the training model is : 3.6474\n",
      "[5000/50000]\n",
      "Loss of the training model is : 3.5892\n",
      "[6000/50000]\n",
      "Loss of the training model is : 3.5467\n",
      "[7000/50000]\n",
      "Loss of the training model is : 3.4922\n",
      "[8000/50000]\n",
      "Loss of the training model is : 3.4416\n",
      "[9000/50000]\n",
      "Loss of the training model is : 3.3650\n",
      "[10000/50000]\n",
      "Loss of the training model is : 3.3034\n",
      "[11000/50000]\n",
      "Loss of the training model is : 3.3045\n",
      "[12000/50000]\n",
      "Loss of the training model is : 3.2306\n",
      "2008.0/10000.0\n",
      "For epoch 2 the test accuracy over the whole test set is 20.08 %\n",
      "Epoch :  3\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.2219\n",
      "[2000/50000]\n",
      "Loss of the training model is : 3.1941\n",
      "[3000/50000]\n",
      "Loss of the training model is : 3.1472\n",
      "[4000/50000]\n",
      "Loss of the training model is : 3.0963\n",
      "[5000/50000]\n",
      "Loss of the training model is : 3.0253\n",
      "[6000/50000]\n",
      "Loss of the training model is : 3.0382\n",
      "[7000/50000]\n",
      "Loss of the training model is : 2.9582\n",
      "[8000/50000]\n",
      "Loss of the training model is : 2.9346\n",
      "[9000/50000]\n",
      "Loss of the training model is : 2.8852\n",
      "[10000/50000]\n",
      "Loss of the training model is : 2.8685\n",
      "[11000/50000]\n",
      "Loss of the training model is : 2.8273\n",
      "[12000/50000]\n",
      "Loss of the training model is : 2.7590\n",
      "2915.0/10000.0\n",
      "For epoch 3 the test accuracy over the whole test set is 29.15 %\n",
      "Epoch :  4\n",
      "[1000/50000]\n",
      "Loss of the training model is : 2.7175\n",
      "[2000/50000]\n",
      "Loss of the training model is : 2.7225\n",
      "[3000/50000]\n",
      "Loss of the training model is : 2.6589\n",
      "[4000/50000]\n",
      "Loss of the training model is : 2.6974\n",
      "[5000/50000]\n",
      "Loss of the training model is : 2.6557\n",
      "[6000/50000]\n",
      "Loss of the training model is : 2.6258\n",
      "[7000/50000]\n",
      "Loss of the training model is : 2.6403\n",
      "[8000/50000]\n",
      "Loss of the training model is : 2.5701\n",
      "[9000/50000]\n",
      "Loss of the training model is : 2.5523\n",
      "[10000/50000]\n",
      "Loss of the training model is : 2.5551\n",
      "[11000/50000]\n",
      "Loss of the training model is : 2.5121\n",
      "[12000/50000]\n",
      "Loss of the training model is : 2.4959\n",
      "3367.0/10000.0\n",
      "For epoch 4 the test accuracy over the whole test set is 33.67 %\n",
      "Epoch :  5\n",
      "[1000/50000]\n",
      "Loss of the training model is : 2.4287\n",
      "[2000/50000]\n",
      "Loss of the training model is : 2.4043\n",
      "[3000/50000]\n",
      "Loss of the training model is : 2.4175\n",
      "[4000/50000]\n",
      "Loss of the training model is : 2.3947\n",
      "[5000/50000]\n",
      "Loss of the training model is : 2.4407\n",
      "[6000/50000]\n",
      "Loss of the training model is : 2.4332\n",
      "[7000/50000]\n",
      "Loss of the training model is : 2.3908\n",
      "[8000/50000]\n",
      "Loss of the training model is : 2.3335\n",
      "[9000/50000]\n",
      "Loss of the training model is : 2.3276\n",
      "[10000/50000]\n",
      "Loss of the training model is : 2.3174\n",
      "[11000/50000]\n",
      "Loss of the training model is : 2.2858\n",
      "[12000/50000]\n",
      "Loss of the training model is : 2.2912\n",
      "3597.0/10000.0\n",
      "For epoch 5 the test accuracy over the whole test set is 35.97 %\n",
      "Epoch :  6\n",
      "[1000/50000]\n",
      "Loss of the training model is : 1.9789\n",
      "[2000/50000]\n",
      "Loss of the training model is : 1.9534\n",
      "[3000/50000]\n",
      "Loss of the training model is : 2.0102\n",
      "[4000/50000]\n",
      "Loss of the training model is : 1.9739\n",
      "[5000/50000]\n",
      "Loss of the training model is : 1.9388\n",
      "[6000/50000]\n",
      "Loss of the training model is : 2.0430\n",
      "[7000/50000]\n",
      "Loss of the training model is : 2.0031\n",
      "[8000/50000]\n",
      "Loss of the training model is : 1.9548\n",
      "[9000/50000]\n",
      "Loss of the training model is : 1.9482\n",
      "[10000/50000]\n",
      "Loss of the training model is : 2.0126\n",
      "[11000/50000]\n",
      "Loss of the training model is : 1.9826\n",
      "[12000/50000]\n",
      "Loss of the training model is : 1.9926\n",
      "4052.0/10000.0\n",
      "For epoch 6 the test accuracy over the whole test set is 40.52 %\n",
      "Epoch :  7\n",
      "[1000/50000]\n",
      "Loss of the training model is : 1.5864\n",
      "[2000/50000]\n",
      "Loss of the training model is : 1.6034\n",
      "[3000/50000]\n",
      "Loss of the training model is : 1.6243\n",
      "[4000/50000]\n",
      "Loss of the training model is : 1.6359\n",
      "[5000/50000]\n",
      "Loss of the training model is : 1.6406\n",
      "[6000/50000]\n",
      "Loss of the training model is : 1.5839\n",
      "[7000/50000]\n",
      "Loss of the training model is : 1.6538\n",
      "[8000/50000]\n",
      "Loss of the training model is : 1.6328\n",
      "[9000/50000]\n",
      "Loss of the training model is : 1.5813\n",
      "[10000/50000]\n",
      "Loss of the training model is : 1.6634\n",
      "[11000/50000]\n",
      "Loss of the training model is : 1.7202\n",
      "[12000/50000]\n",
      "Loss of the training model is : 1.6378\n",
      "4678.0/10000.0\n",
      "For epoch 7 the test accuracy over the whole test set is 46.78 %\n",
      "Epoch :  8\n",
      "[1000/50000]\n",
      "Loss of the training model is : 1.2211\n",
      "[2000/50000]\n",
      "Loss of the training model is : 1.1847\n",
      "[3000/50000]\n",
      "Loss of the training model is : 1.2083\n",
      "[4000/50000]\n",
      "Loss of the training model is : 1.2099\n",
      "[5000/50000]\n",
      "Loss of the training model is : 1.2250\n",
      "[6000/50000]\n",
      "Loss of the training model is : 1.2305\n",
      "[7000/50000]\n",
      "Loss of the training model is : 1.2752\n",
      "[8000/50000]\n",
      "Loss of the training model is : 1.2697\n",
      "[9000/50000]\n",
      "Loss of the training model is : 1.2664\n",
      "[10000/50000]\n",
      "Loss of the training model is : 1.2616\n",
      "[11000/50000]\n",
      "Loss of the training model is : 1.2480\n",
      "[12000/50000]\n",
      "Loss of the training model is : 1.2951\n",
      "4970.0/10000.0\n",
      "For epoch 8 the test accuracy over the whole test set is 49.70 %\n",
      "Epoch :  9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(\u001b[39m25\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m#outputs_googLeNet = torch.add(torch.add(outputs[0]*0.4, outputs[1]*0.3).cuda(),outputs[2]*0.3).cuda()\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs,labels)\n\u001b[1;32m---> 34\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m running_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\get2b\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\get2b\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),r'C:\\Users\\get2b\\Desktop\\Arav\\Neural Networks\\Pytorch implementation\\All Models Saved\\AlexNet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a96baaf79f14888d285bcba7f550195ef41d580783ed1a8999b5f8e1380f57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
