{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 56, 56])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand([32,3,224,224])\n",
    "a = nn.Sequential(nn.Conv2d(3,64,7,2,3),\n",
    "                    nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                    nn.MaxPool2d(3,2,1))(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)).cuda()])\n",
    "trainset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data',train=True,download=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data',train=False,download=False,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=1000,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXt(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3,64,7,2,3),\n",
    "                                   nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                                   nn.MaxPool2d(3,2,1))\n",
    "        \n",
    "        self.conv2d_x = nn.Sequential(nn.Conv2d(64,128,1,1),nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                   nn.Conv2d(128,128,3,1,1,groups=32),nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                   nn.Conv2d(128,256,1,1),nn.BatchNorm2d(256),nn.ReLU())\n",
    "        self.conv2_x = nn.Sequential(nn.Conv2d(256,128,1,1),nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                   nn.Conv2d(128,128,3,1,1,groups=32),nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                   nn.Conv2d(128,256,1,1),nn.BatchNorm2d(256),nn.ReLU())\n",
    "        \n",
    "        self.conv3d_x = nn.Sequential(nn.Conv2d(256,256,1,1),nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                   nn.Conv2d(256,256,3,2,1,groups=32),nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                   nn.Conv2d(256,512,1,1),nn.BatchNorm2d(512),nn.ReLU())\n",
    "        self.conv3_x = nn.Sequential(nn.Conv2d(512,256,1,1),nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                   nn.Conv2d(256,256,3,1,1,groups=32),nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                   nn.Conv2d(256,512,1,1),nn.BatchNorm2d(512),nn.ReLU())\n",
    "        \n",
    "        self.conv4d_x = nn.Sequential(nn.Conv2d(512,512,1,1),nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                   nn.Conv2d(512,512,3,2,1,groups=32),nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                   nn.Conv2d(512,1024,1,1),nn.BatchNorm2d(1024),nn.ReLU())\n",
    "        self.conv4_x = nn.Sequential(nn.Conv2d(1024,512,1,1),nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                   nn.Conv2d(512,512,3,1,1,groups=32),nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                   nn.Conv2d(512,1024,1,1),nn.BatchNorm2d(1024),nn.ReLU())\n",
    "        \n",
    "        self.conv5d_x = nn.Sequential(nn.Conv2d(1024,1024,1,1),nn.BatchNorm2d(1024),nn.ReLU(),\n",
    "                                   nn.Conv2d(1024,1024,3,2,1,groups=32),nn.BatchNorm2d(1024),nn.ReLU(),\n",
    "                                   nn.Conv2d(1024,2048,1,1),nn.BatchNorm2d(2048),nn.ReLU())\n",
    "        self.conv5_x = nn.Sequential(nn.Conv2d(2048,1024,1,1),nn.BatchNorm2d(1024),nn.ReLU(),\n",
    "                                   nn.Conv2d(1024,1024,3,1,1,groups=32),nn.BatchNorm2d(1024),nn.ReLU(),\n",
    "                                   nn.Conv2d(1024,2048,1,1),nn.BatchNorm2d(2048),nn.ReLU())\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(1,1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2048,1000)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x1 = self.conv2d_x(x)\n",
    "        x1 = self.conv2_x(x1)\n",
    "        x = nn.Conv2d(64,256,1,1).cuda()(x)\n",
    "        x = torch.add(x,x1).cuda()\n",
    "        \n",
    "        x1 = self.conv2_x(x)\n",
    "        x2 = self.conv3d_x(x1)\n",
    "        x1 = nn.Conv2d(256,512,1,1).cuda()(x)\n",
    "        x1 = nn.MaxPool2d(3,2,1).cuda()(x1)\n",
    "        x2 = torch.add(x1,x2).cuda()\n",
    "        \n",
    "        x3 = self.conv3_x(x2)\n",
    "        x3 = self.conv3_x(x3)\n",
    "        x3 = torch.add(x2,x3).cuda()\n",
    "        \n",
    "        x4 = self.conv3_x(x3)\n",
    "        x5 = self.conv4d_x(x4)\n",
    "        x3 = nn.MaxPool2d(3,2,1).cuda()(x3)\n",
    "        x3 = nn.Conv2d(512,1024,1,1).cuda()(x3)\n",
    "        x5 = torch.add(x3,x5).cuda()\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x6 = self.conv4_x(x6)\n",
    "        x5 = torch.add(x5,x6)\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x6 = self.conv4_x(x6)\n",
    "        x5 = torch.add(x5,x6).cuda()\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x7 = self.conv5d_x(x6)\n",
    "        x6 = nn.MaxPool2d(3,2,1).cuda()(x5)\n",
    "        x6 = nn.Conv2d(1024,2048,1,1).cuda()(x6)\n",
    "        x7 = torch.add(x6,x7).cuda()\n",
    "        \n",
    "        x8 = self.conv5_x(x7)\n",
    "        x8 = self.conv5_x(x8)\n",
    "        x8 = torch.add(x7,x8).cuda()\n",
    "\n",
    "        x8 = self.avgpool(x8)\n",
    "        x8 = nn.Flatten().cuda()(x8)\n",
    "        x8 = self.linear(x8)\n",
    "        \n",
    "        return x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find a saved weights of the model\n"
     ]
    }
   ],
   "source": [
    "model = ResNeXt().cuda()\n",
    "try:\n",
    "    model.load_state_dict(torch.load('C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\Neural Networks\\\\Pytorch implementation\\\\All Models Saved\\\\ResNext50.pth'))\n",
    "except:\n",
    "    print(\"Couldn\\'t find a saved weights of the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 16, 16]           9,472\n",
      "       BatchNorm2d-2           [64, 64, 16, 16]             128\n",
      "              ReLU-3           [64, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [64, 64, 8, 8]               0\n",
      "            Conv2d-5            [64, 128, 8, 8]           8,320\n",
      "       BatchNorm2d-6            [64, 128, 8, 8]             256\n",
      "              ReLU-7            [64, 128, 8, 8]               0\n",
      "            Conv2d-8            [64, 128, 8, 8]           4,736\n",
      "       BatchNorm2d-9            [64, 128, 8, 8]             256\n",
      "             ReLU-10            [64, 128, 8, 8]               0\n",
      "           Conv2d-11            [64, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-12            [64, 256, 8, 8]             512\n",
      "             ReLU-13            [64, 256, 8, 8]               0\n",
      "           Conv2d-14            [64, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-15            [64, 128, 8, 8]             256\n",
      "             ReLU-16            [64, 128, 8, 8]               0\n",
      "           Conv2d-17            [64, 128, 8, 8]           4,736\n",
      "      BatchNorm2d-18            [64, 128, 8, 8]             256\n",
      "             ReLU-19            [64, 128, 8, 8]               0\n",
      "           Conv2d-20            [64, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-21            [64, 256, 8, 8]             512\n",
      "             ReLU-22            [64, 256, 8, 8]               0\n",
      "           Conv2d-23            [64, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-24            [64, 128, 8, 8]             256\n",
      "             ReLU-25            [64, 128, 8, 8]               0\n",
      "           Conv2d-26            [64, 128, 8, 8]           4,736\n",
      "      BatchNorm2d-27            [64, 128, 8, 8]             256\n",
      "             ReLU-28            [64, 128, 8, 8]               0\n",
      "           Conv2d-29            [64, 256, 8, 8]          33,024\n",
      "      BatchNorm2d-30            [64, 256, 8, 8]             512\n",
      "             ReLU-31            [64, 256, 8, 8]               0\n",
      "           Conv2d-32            [64, 256, 8, 8]          65,792\n",
      "      BatchNorm2d-33            [64, 256, 8, 8]             512\n",
      "             ReLU-34            [64, 256, 8, 8]               0\n",
      "           Conv2d-35            [64, 256, 4, 4]          18,688\n",
      "      BatchNorm2d-36            [64, 256, 4, 4]             512\n",
      "             ReLU-37            [64, 256, 4, 4]               0\n",
      "           Conv2d-38            [64, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-39            [64, 512, 4, 4]           1,024\n",
      "             ReLU-40            [64, 512, 4, 4]               0\n",
      "           Conv2d-41            [64, 256, 4, 4]         131,328\n",
      "      BatchNorm2d-42            [64, 256, 4, 4]             512\n",
      "             ReLU-43            [64, 256, 4, 4]               0\n",
      "           Conv2d-44            [64, 256, 4, 4]          18,688\n",
      "      BatchNorm2d-45            [64, 256, 4, 4]             512\n",
      "             ReLU-46            [64, 256, 4, 4]               0\n",
      "           Conv2d-47            [64, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-48            [64, 512, 4, 4]           1,024\n",
      "             ReLU-49            [64, 512, 4, 4]               0\n",
      "           Conv2d-50            [64, 256, 4, 4]         131,328\n",
      "      BatchNorm2d-51            [64, 256, 4, 4]             512\n",
      "             ReLU-52            [64, 256, 4, 4]               0\n",
      "           Conv2d-53            [64, 256, 4, 4]          18,688\n",
      "      BatchNorm2d-54            [64, 256, 4, 4]             512\n",
      "             ReLU-55            [64, 256, 4, 4]               0\n",
      "           Conv2d-56            [64, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-57            [64, 512, 4, 4]           1,024\n",
      "             ReLU-58            [64, 512, 4, 4]               0\n",
      "           Conv2d-59            [64, 256, 4, 4]         131,328\n",
      "      BatchNorm2d-60            [64, 256, 4, 4]             512\n",
      "             ReLU-61            [64, 256, 4, 4]               0\n",
      "           Conv2d-62            [64, 256, 4, 4]          18,688\n",
      "      BatchNorm2d-63            [64, 256, 4, 4]             512\n",
      "             ReLU-64            [64, 256, 4, 4]               0\n",
      "           Conv2d-65            [64, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-66            [64, 512, 4, 4]           1,024\n",
      "             ReLU-67            [64, 512, 4, 4]               0\n",
      "           Conv2d-68            [64, 512, 4, 4]         262,656\n",
      "      BatchNorm2d-69            [64, 512, 4, 4]           1,024\n",
      "             ReLU-70            [64, 512, 4, 4]               0\n",
      "           Conv2d-71            [64, 512, 2, 2]          74,240\n",
      "      BatchNorm2d-72            [64, 512, 2, 2]           1,024\n",
      "             ReLU-73            [64, 512, 2, 2]               0\n",
      "           Conv2d-74           [64, 1024, 2, 2]         525,312\n",
      "      BatchNorm2d-75           [64, 1024, 2, 2]           2,048\n",
      "             ReLU-76           [64, 1024, 2, 2]               0\n",
      "           Conv2d-77            [64, 512, 2, 2]         524,800\n",
      "      BatchNorm2d-78            [64, 512, 2, 2]           1,024\n",
      "             ReLU-79            [64, 512, 2, 2]               0\n",
      "           Conv2d-80            [64, 512, 2, 2]          74,240\n",
      "      BatchNorm2d-81            [64, 512, 2, 2]           1,024\n",
      "             ReLU-82            [64, 512, 2, 2]               0\n",
      "           Conv2d-83           [64, 1024, 2, 2]         525,312\n",
      "      BatchNorm2d-84           [64, 1024, 2, 2]           2,048\n",
      "             ReLU-85           [64, 1024, 2, 2]               0\n",
      "           Conv2d-86            [64, 512, 2, 2]         524,800\n",
      "      BatchNorm2d-87            [64, 512, 2, 2]           1,024\n",
      "             ReLU-88            [64, 512, 2, 2]               0\n",
      "           Conv2d-89            [64, 512, 2, 2]          74,240\n",
      "      BatchNorm2d-90            [64, 512, 2, 2]           1,024\n",
      "             ReLU-91            [64, 512, 2, 2]               0\n",
      "           Conv2d-92           [64, 1024, 2, 2]         525,312\n",
      "      BatchNorm2d-93           [64, 1024, 2, 2]           2,048\n",
      "             ReLU-94           [64, 1024, 2, 2]               0\n",
      "           Conv2d-95            [64, 512, 2, 2]         524,800\n",
      "      BatchNorm2d-96            [64, 512, 2, 2]           1,024\n",
      "             ReLU-97            [64, 512, 2, 2]               0\n",
      "           Conv2d-98            [64, 512, 2, 2]          74,240\n",
      "      BatchNorm2d-99            [64, 512, 2, 2]           1,024\n",
      "            ReLU-100            [64, 512, 2, 2]               0\n",
      "          Conv2d-101           [64, 1024, 2, 2]         525,312\n",
      "     BatchNorm2d-102           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-103           [64, 1024, 2, 2]               0\n",
      "          Conv2d-104            [64, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-105            [64, 512, 2, 2]           1,024\n",
      "            ReLU-106            [64, 512, 2, 2]               0\n",
      "          Conv2d-107            [64, 512, 2, 2]          74,240\n",
      "     BatchNorm2d-108            [64, 512, 2, 2]           1,024\n",
      "            ReLU-109            [64, 512, 2, 2]               0\n",
      "          Conv2d-110           [64, 1024, 2, 2]         525,312\n",
      "     BatchNorm2d-111           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-112           [64, 1024, 2, 2]               0\n",
      "          Conv2d-113            [64, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-114            [64, 512, 2, 2]           1,024\n",
      "            ReLU-115            [64, 512, 2, 2]               0\n",
      "          Conv2d-116            [64, 512, 2, 2]          74,240\n",
      "     BatchNorm2d-117            [64, 512, 2, 2]           1,024\n",
      "            ReLU-118            [64, 512, 2, 2]               0\n",
      "          Conv2d-119           [64, 1024, 2, 2]         525,312\n",
      "     BatchNorm2d-120           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-121           [64, 1024, 2, 2]               0\n",
      "          Conv2d-122           [64, 1024, 2, 2]       1,049,600\n",
      "     BatchNorm2d-123           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-124           [64, 1024, 2, 2]               0\n",
      "          Conv2d-125           [64, 1024, 1, 1]         295,936\n",
      "     BatchNorm2d-126           [64, 1024, 1, 1]           2,048\n",
      "            ReLU-127           [64, 1024, 1, 1]               0\n",
      "          Conv2d-128           [64, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-129           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-130           [64, 2048, 1, 1]               0\n",
      "          Conv2d-131           [64, 1024, 1, 1]       2,098,176\n",
      "     BatchNorm2d-132           [64, 1024, 1, 1]           2,048\n",
      "            ReLU-133           [64, 1024, 1, 1]               0\n",
      "          Conv2d-134           [64, 1024, 1, 1]         295,936\n",
      "     BatchNorm2d-135           [64, 1024, 1, 1]           2,048\n",
      "            ReLU-136           [64, 1024, 1, 1]               0\n",
      "          Conv2d-137           [64, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-138           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-139           [64, 2048, 1, 1]               0\n",
      "          Conv2d-140           [64, 1024, 1, 1]       2,098,176\n",
      "     BatchNorm2d-141           [64, 1024, 1, 1]           2,048\n",
      "            ReLU-142           [64, 1024, 1, 1]               0\n",
      "          Conv2d-143           [64, 1024, 1, 1]         295,936\n",
      "     BatchNorm2d-144           [64, 1024, 1, 1]           2,048\n",
      "            ReLU-145           [64, 1024, 1, 1]               0\n",
      "          Conv2d-146           [64, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-147           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-148           [64, 2048, 1, 1]               0\n",
      "       AvgPool2d-149           [64, 2048, 1, 1]               0\n",
      "          Linear-150                 [64, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 22,282,600\n",
      "Trainable params: 22,282,600\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 388.99\n",
      "Params size (MB): 85.00\n",
      "Estimated Total Size (MB): 474.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model=model,input_size=(3,32,32),batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lr_scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer,\\n                                           base_lr=1e-5,\\n                                           max_lr=1e-2,\\n                                           step_size_up=5,\\n                                           step_size_down=5,\\n                                           cycle_momentum=False,\\n                                           mode=\"exp_range\")'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=0.0001)\n",
    "lr_scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer,\n",
    "                                           base_lr=1e-3,\n",
    "                                           max_lr=1e-1,\n",
    "                                           step_size_up=5,\n",
    "                                           step_size_down=5,\n",
    "                                           cycle_momentum=False,\n",
    "                                           mode=\"exp_range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_every_epoch = []\n",
    "\n",
    "def test():\n",
    "    \n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i,(inputs,labels) in enumerate(test_loader,0):\n",
    "            \n",
    "        inputs,labels = inputs.cuda(),labels.cuda()\n",
    "        ouputs = model(inputs)\n",
    "        _, prediction = torch.max(ouputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        accuracy += (prediction == labels).sum().item()\n",
    "    \n",
    "    print(\"Accuracy of images {}/{}\".format(accuracy,total))\n",
    "    print(\"Test accuracy of this epoch is : {}\".format((accuracy/total)*100))\n",
    "    \n",
    "        \n",
    "def train(epoch):\n",
    "    for ep in range(0,epoch):\n",
    "        losses = 0.0\n",
    "        \n",
    "        for i,(inputs,labels) in enumerate(train_loader,0):\n",
    "            \n",
    "            inputs,labels = inputs.cuda(),labels.cuda()\n",
    "            optimizer.zero_grad(set_to_none=False)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs,labels)\n",
    "            losses += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        print(\"Training loss for the Epoch is {} : {}\".format(ep+1,losses/i+1))\n",
    "        loss_info = \"Training loss for Epoch \"+str(ep+1)+\" is \"+str((losses/i+1))\n",
    "        loss_every_epoch.append(loss_info)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for the Epoch is 1 : 6.325426419168978\n",
      "Accuracy of images 106.0/10000.0\n",
      "Test accuracy of this epoch is : 1.06\n",
      "Training loss for the Epoch is 2 : 5.569156669318752\n",
      "Accuracy of images 260.0/10000.0\n",
      "Test accuracy of this epoch is : 2.6\n",
      "Training loss for the Epoch is 3 : 5.340376798077826\n",
      "Accuracy of images 448.0/10000.0\n",
      "Test accuracy of this epoch is : 4.4799999999999995\n",
      "Training loss for the Epoch is 4 : 5.160644457679094\n",
      "Accuracy of images 606.0/10000.0\n",
      "Test accuracy of this epoch is : 6.0600000000000005\n",
      "Training loss for the Epoch is 5 : 5.050976864049132\n",
      "Accuracy of images 807.0/10000.0\n",
      "Test accuracy of this epoch is : 8.07\n",
      "Training loss for the Epoch is 6 : 4.96563289473823\n",
      "Accuracy of images 861.0/10000.0\n",
      "Test accuracy of this epoch is : 8.61\n",
      "Training loss for the Epoch is 7 : 4.8680844316104\n",
      "Accuracy of images 916.0/10000.0\n",
      "Test accuracy of this epoch is : 9.16\n",
      "Training loss for the Epoch is 8 : 4.782733306689391\n",
      "Accuracy of images 1096.0/10000.0\n",
      "Test accuracy of this epoch is : 10.96\n",
      "Training loss for the Epoch is 9 : 4.696375438406892\n",
      "Accuracy of images 1341.0/10000.0\n",
      "Test accuracy of this epoch is : 13.41\n",
      "Training loss for the Epoch is 10 : 4.609462312180621\n",
      "Accuracy of images 1420.0/10000.0\n",
      "Test accuracy of this epoch is : 14.2\n",
      "Training loss for the Epoch is 11 : 4.523330910715648\n",
      "Accuracy of images 1538.0/10000.0\n",
      "Test accuracy of this epoch is : 15.379999999999999\n",
      "Training loss for the Epoch is 12 : 4.4447878182468585\n",
      "Accuracy of images 1690.0/10000.0\n",
      "Test accuracy of this epoch is : 16.900000000000002\n",
      "Training loss for the Epoch is 13 : 4.378092301792433\n",
      "Accuracy of images 1794.0/10000.0\n",
      "Test accuracy of this epoch is : 17.94\n",
      "Training loss for the Epoch is 14 : 4.309615644999564\n",
      "Accuracy of images 1927.0/10000.0\n",
      "Test accuracy of this epoch is : 19.27\n",
      "Training loss for the Epoch is 15 : 4.249510728748141\n",
      "Accuracy of images 1887.0/10000.0\n",
      "Test accuracy of this epoch is : 18.87\n",
      "Training loss for the Epoch is 16 : 4.197187704252372\n",
      "Accuracy of images 2065.0/10000.0\n",
      "Test accuracy of this epoch is : 20.65\n",
      "Training loss for the Epoch is 17 : 4.143802920315849\n",
      "Accuracy of images 2071.0/10000.0\n",
      "Test accuracy of this epoch is : 20.71\n",
      "Training loss for the Epoch is 18 : 4.087444491942011\n",
      "Accuracy of images 2149.0/10000.0\n",
      "Test accuracy of this epoch is : 21.490000000000002\n",
      "Training loss for the Epoch is 19 : 4.044070431883906\n",
      "Accuracy of images 2197.0/10000.0\n",
      "Test accuracy of this epoch is : 21.97\n",
      "Training loss for the Epoch is 20 : 3.997769236106726\n",
      "Accuracy of images 2298.0/10000.0\n",
      "Test accuracy of this epoch is : 22.98\n",
      "Training loss for the Epoch is 21 : 3.954511085331974\n",
      "Accuracy of images 2274.0/10000.0\n",
      "Test accuracy of this epoch is : 22.74\n",
      "Training loss for the Epoch is 22 : 3.9132312437025747\n",
      "Accuracy of images 2301.0/10000.0\n",
      "Test accuracy of this epoch is : 23.01\n",
      "Training loss for the Epoch is 23 : 3.8678735007702465\n",
      "Accuracy of images 2395.0/10000.0\n",
      "Test accuracy of this epoch is : 23.95\n",
      "Training loss for the Epoch is 24 : 3.8284354939503493\n",
      "Accuracy of images 2410.0/10000.0\n",
      "Test accuracy of this epoch is : 24.099999999999998\n",
      "Training loss for the Epoch is 25 : 3.782816981841904\n",
      "Accuracy of images 2479.0/10000.0\n",
      "Test accuracy of this epoch is : 24.79\n"
     ]
    }
   ],
   "source": [
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\Neural Networks\\\\Pytorch implementation\\\\All Models Saved\\\\ResNext50.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a96baaf79f14888d285bcba7f550195ef41d580783ed1a8999b5f8e1380f57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
