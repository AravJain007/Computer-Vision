{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)).cuda()])\n",
    "trainset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data',train=True,download=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=32,shuffle=True,num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR100(root='C:\\\\Users\\\\get2b\\\\Desktop\\\\Arav\\\\AI ML\\\\DATA SETS\\\\pytorch Data',train=False,download=False,transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=1000,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0275,  0.0275,  0.0588,  ...,  0.1686,  0.1216,  0.0667],\n",
       "          [-0.0510,  0.0039,  0.0510,  ...,  0.1843,  0.1451,  0.1137],\n",
       "          [-0.0431,  0.0118,  0.0667,  ...,  0.2235,  0.2000,  0.1608],\n",
       "          ...,\n",
       "          [ 0.0118,  0.1529,  0.1843,  ...,  0.8353,  0.8667,  0.7569],\n",
       "          [ 0.1765,  0.1922,  0.2235,  ...,  0.7804,  0.8118,  0.8118],\n",
       "          [ 0.1529,  0.1451,  0.2235,  ...,  0.8039,  0.9059,  0.9059]],\n",
       " \n",
       "         [[ 0.5216,  0.5843,  0.6157,  ...,  0.7647,  0.7255,  0.6706],\n",
       "          [ 0.5216,  0.5765,  0.6157,  ...,  0.7569,  0.7412,  0.7020],\n",
       "          [ 0.5451,  0.6000,  0.6314,  ...,  0.7882,  0.7725,  0.7333],\n",
       "          ...,\n",
       "          [-0.6706, -0.8667, -0.8745,  ...,  0.8118,  0.8824,  0.5451],\n",
       "          [-0.8353, -0.8667, -0.8039,  ...,  0.7647,  0.8902,  0.8510],\n",
       "          [-0.9059, -0.9137, -0.8824,  ...,  0.8588,  0.8588,  0.8039]],\n",
       " \n",
       "         [[ 0.7333,  0.7882,  0.7882,  ...,  0.8510,  0.8353,  0.8039],\n",
       "          [ 0.7255,  0.7647,  0.7882,  ...,  0.8353,  0.8353,  0.8196],\n",
       "          [ 0.7490,  0.7882,  0.7961,  ...,  0.8510,  0.8588,  0.8431],\n",
       "          ...,\n",
       "          [-0.5373, -0.7020, -0.7647,  ...,  0.7647,  0.8275,  0.5765],\n",
       "          [-0.6863, -0.7412, -0.7176,  ...,  0.7333,  0.8275,  0.8039],\n",
       "          [-0.8039, -0.8196, -0.7725,  ...,  0.8196,  0.8510,  0.8118]]]),\n",
       " 11)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.top = nn.Sequential(nn.Conv2d(3,64,7,2,3),\n",
    "                                 nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                                 nn.MaxPool2d(3,2,1))\n",
    "        self.conv2d_x = nn.Sequential(nn.Conv2d(64,64,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=64),nn.ReLU(),\n",
    "                                    nn.Conv2d(64,64,3,2,1),\n",
    "                                    nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                                    nn.Conv2d(64,256,1,1),\n",
    "                                    nn.BatchNorm2d(256),nn.ReLU())\n",
    "        self.conv2_x = nn.Sequential(nn.Conv2d(256,64,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=64),nn.ReLU(),\n",
    "                                    nn.Conv2d(64,64,3,1,1),\n",
    "                                    nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                                    nn.Conv2d(64,256,1,1),\n",
    "                                    nn.BatchNorm2d(256),nn.ReLU())\n",
    "        \n",
    "        self.conv3d_x = nn.Sequential(nn.Conv2d(256,128,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=128),nn.ReLU(),\n",
    "                                    nn.Conv2d(128,128,3,1,1),\n",
    "                                    nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                    nn.Conv2d(128,512,1,1),\n",
    "                                    nn.BatchNorm2d(512),nn.ReLU())\n",
    "        self.conv3_x = nn.Sequential(nn.Conv2d(512,128,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=128),nn.ReLU(),\n",
    "                                    nn.Conv2d(128,128,3,1,1),\n",
    "                                    nn.BatchNorm2d(128),nn.ReLU(),\n",
    "                                    nn.Conv2d(128,512,1,1),\n",
    "                                    nn.BatchNorm2d(512),nn.ReLU())\n",
    "        \n",
    "        self.conv4d_x = nn.Sequential(nn.Conv2d(512,256,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=256),nn.ReLU(),\n",
    "                                    nn.Conv2d(256,256,3,2,1),\n",
    "                                    nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                    nn.Conv2d(256,1024,1,1),\n",
    "                                    nn.BatchNorm2d(1024),nn.ReLU())\n",
    "        self.conv4_x = nn.Sequential(nn.Conv2d(1024,256,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=256),nn.ReLU(),\n",
    "                                    nn.Conv2d(256,256,3,1,1),\n",
    "                                    nn.BatchNorm2d(256),nn.ReLU(),\n",
    "                                    nn.Conv2d(256,1024,1,1),\n",
    "                                    nn.BatchNorm2d(1024),nn.ReLU())\n",
    "        \n",
    "        self.conv5d_x = nn.Sequential(nn.Conv2d(1024,512,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=512),nn.ReLU(),\n",
    "                                    nn.Conv2d(512,512,3,2,1),\n",
    "                                    nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                    nn.Conv2d(512,2048,1,1),\n",
    "                                    nn.BatchNorm2d(2048),nn.ReLU())\n",
    "        self.conv5_x = nn.Sequential(nn.Conv2d(2048,512,1,1),\n",
    "                                    nn.BatchNorm2d(num_features=512),nn.ReLU(),\n",
    "                                    nn.Conv2d(512,512,3,1,1),\n",
    "                                    nn.BatchNorm2d(512),nn.ReLU(),\n",
    "                                    nn.Conv2d(512,2048,1,1),\n",
    "                                    nn.BatchNorm2d(2048),nn.ReLU())\n",
    "        self.avgpool = nn.AvgPool2d(7,1)\n",
    "        self.linear = nn.Linear(2048,1000)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.top(x)\n",
    "        x1 = self.conv2d_x(x)\n",
    "        x1 = self.conv2_x(x1)\n",
    "        x = nn.MaxPool2d(3,2,1).cuda()(x)\n",
    "        x = nn.Conv2d(64,256,1,1).cuda()(x)\n",
    "        x = torch.add(x,x1).cuda()\n",
    "        \n",
    "        x1 = self.conv2_x(x)\n",
    "        x2 = self.conv3d_x(x1)\n",
    "        x1 = nn.Conv2d(256,512,1,1).cuda()(x)\n",
    "        x2 = torch.add(x1,x2).cuda()\n",
    "        \n",
    "        x3 = self.conv3_x(x2)\n",
    "        x3 = self.conv3_x(x3)\n",
    "        x3 = torch.add(x2,x3).cuda()\n",
    "        \n",
    "        x4 = self.conv3_x(x3)\n",
    "        x5 = self.conv4d_x(x4)\n",
    "        x3 = nn.MaxPool2d(3,2,1).cuda()(x3)\n",
    "        x3 = nn.Conv2d(512,1024,1,1).cuda()(x3)\n",
    "        x5 = torch.add(x3,x5).cuda()\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x6 = self.conv4_x(x6)\n",
    "        x5 = torch.add(x5,x6)\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x6 = self.conv4_x(x6)\n",
    "        x5 = torch.add(x5,x6).cuda()\n",
    "        \n",
    "        x6 = self.conv4_x(x5)\n",
    "        x7 = self.conv5d_x(x6)\n",
    "        x6 = nn.MaxPool2d(3,2,1).cuda()(x5)\n",
    "        x6 = nn.Conv2d(1024,2048,1,1).cuda()(x6)\n",
    "        x7 = torch.add(x6,x7).cuda()\n",
    "        \n",
    "        x8 = self.conv5_x(x7)\n",
    "        x8 = self.conv5_x(x8)\n",
    "        x8 = torch.add(x7,x8).cuda()\n",
    "\n",
    "        # x8 = self.avgpool(x8)     This layer is required when we are training on ImageNet 1000\n",
    "        x8 = nn.Flatten().cuda()(x8)\n",
    "        x8 = self.linear(x8)\n",
    "        \n",
    "        return x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 16, 16]           9,472\n",
      "       BatchNorm2d-2            [1, 64, 16, 16]             128\n",
      "              ReLU-3            [1, 64, 16, 16]               0\n",
      "         MaxPool2d-4              [1, 64, 8, 8]               0\n",
      "            Conv2d-5              [1, 64, 8, 8]           4,160\n",
      "       BatchNorm2d-6              [1, 64, 8, 8]             128\n",
      "              ReLU-7              [1, 64, 8, 8]               0\n",
      "            Conv2d-8              [1, 64, 4, 4]          36,928\n",
      "       BatchNorm2d-9              [1, 64, 4, 4]             128\n",
      "             ReLU-10              [1, 64, 4, 4]               0\n",
      "           Conv2d-11             [1, 256, 4, 4]          16,640\n",
      "      BatchNorm2d-12             [1, 256, 4, 4]             512\n",
      "             ReLU-13             [1, 256, 4, 4]               0\n",
      "           Conv2d-14              [1, 64, 4, 4]          16,448\n",
      "      BatchNorm2d-15              [1, 64, 4, 4]             128\n",
      "             ReLU-16              [1, 64, 4, 4]               0\n",
      "           Conv2d-17              [1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-18              [1, 64, 4, 4]             128\n",
      "             ReLU-19              [1, 64, 4, 4]               0\n",
      "           Conv2d-20             [1, 256, 4, 4]          16,640\n",
      "      BatchNorm2d-21             [1, 256, 4, 4]             512\n",
      "             ReLU-22             [1, 256, 4, 4]               0\n",
      "           Conv2d-23              [1, 64, 4, 4]          16,448\n",
      "      BatchNorm2d-24              [1, 64, 4, 4]             128\n",
      "             ReLU-25              [1, 64, 4, 4]               0\n",
      "           Conv2d-26              [1, 64, 4, 4]          36,928\n",
      "      BatchNorm2d-27              [1, 64, 4, 4]             128\n",
      "             ReLU-28              [1, 64, 4, 4]               0\n",
      "           Conv2d-29             [1, 256, 4, 4]          16,640\n",
      "      BatchNorm2d-30             [1, 256, 4, 4]             512\n",
      "             ReLU-31             [1, 256, 4, 4]               0\n",
      "           Conv2d-32             [1, 128, 4, 4]          32,896\n",
      "      BatchNorm2d-33             [1, 128, 4, 4]             256\n",
      "             ReLU-34             [1, 128, 4, 4]               0\n",
      "           Conv2d-35             [1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-36             [1, 128, 4, 4]             256\n",
      "             ReLU-37             [1, 128, 4, 4]               0\n",
      "           Conv2d-38             [1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-39             [1, 512, 4, 4]           1,024\n",
      "             ReLU-40             [1, 512, 4, 4]               0\n",
      "           Conv2d-41             [1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-42             [1, 128, 4, 4]             256\n",
      "             ReLU-43             [1, 128, 4, 4]               0\n",
      "           Conv2d-44             [1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-45             [1, 128, 4, 4]             256\n",
      "             ReLU-46             [1, 128, 4, 4]               0\n",
      "           Conv2d-47             [1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-48             [1, 512, 4, 4]           1,024\n",
      "             ReLU-49             [1, 512, 4, 4]               0\n",
      "           Conv2d-50             [1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-51             [1, 128, 4, 4]             256\n",
      "             ReLU-52             [1, 128, 4, 4]               0\n",
      "           Conv2d-53             [1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-54             [1, 128, 4, 4]             256\n",
      "             ReLU-55             [1, 128, 4, 4]               0\n",
      "           Conv2d-56             [1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-57             [1, 512, 4, 4]           1,024\n",
      "             ReLU-58             [1, 512, 4, 4]               0\n",
      "           Conv2d-59             [1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-60             [1, 128, 4, 4]             256\n",
      "             ReLU-61             [1, 128, 4, 4]               0\n",
      "           Conv2d-62             [1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-63             [1, 128, 4, 4]             256\n",
      "             ReLU-64             [1, 128, 4, 4]               0\n",
      "           Conv2d-65             [1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-66             [1, 512, 4, 4]           1,024\n",
      "             ReLU-67             [1, 512, 4, 4]               0\n",
      "           Conv2d-68             [1, 256, 4, 4]         131,328\n",
      "      BatchNorm2d-69             [1, 256, 4, 4]             512\n",
      "             ReLU-70             [1, 256, 4, 4]               0\n",
      "           Conv2d-71             [1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-72             [1, 256, 2, 2]             512\n",
      "             ReLU-73             [1, 256, 2, 2]               0\n",
      "           Conv2d-74            [1, 1024, 2, 2]         263,168\n",
      "      BatchNorm2d-75            [1, 1024, 2, 2]           2,048\n",
      "             ReLU-76            [1, 1024, 2, 2]               0\n",
      "           Conv2d-77             [1, 256, 2, 2]         262,400\n",
      "      BatchNorm2d-78             [1, 256, 2, 2]             512\n",
      "             ReLU-79             [1, 256, 2, 2]               0\n",
      "           Conv2d-80             [1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-81             [1, 256, 2, 2]             512\n",
      "             ReLU-82             [1, 256, 2, 2]               0\n",
      "           Conv2d-83            [1, 1024, 2, 2]         263,168\n",
      "      BatchNorm2d-84            [1, 1024, 2, 2]           2,048\n",
      "             ReLU-85            [1, 1024, 2, 2]               0\n",
      "           Conv2d-86             [1, 256, 2, 2]         262,400\n",
      "      BatchNorm2d-87             [1, 256, 2, 2]             512\n",
      "             ReLU-88             [1, 256, 2, 2]               0\n",
      "           Conv2d-89             [1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-90             [1, 256, 2, 2]             512\n",
      "             ReLU-91             [1, 256, 2, 2]               0\n",
      "           Conv2d-92            [1, 1024, 2, 2]         263,168\n",
      "      BatchNorm2d-93            [1, 1024, 2, 2]           2,048\n",
      "             ReLU-94            [1, 1024, 2, 2]               0\n",
      "           Conv2d-95             [1, 256, 2, 2]         262,400\n",
      "      BatchNorm2d-96             [1, 256, 2, 2]             512\n",
      "             ReLU-97             [1, 256, 2, 2]               0\n",
      "           Conv2d-98             [1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-99             [1, 256, 2, 2]             512\n",
      "            ReLU-100             [1, 256, 2, 2]               0\n",
      "          Conv2d-101            [1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-102            [1, 1024, 2, 2]           2,048\n",
      "            ReLU-103            [1, 1024, 2, 2]               0\n",
      "          Conv2d-104             [1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-105             [1, 256, 2, 2]             512\n",
      "            ReLU-106             [1, 256, 2, 2]               0\n",
      "          Conv2d-107             [1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-108             [1, 256, 2, 2]             512\n",
      "            ReLU-109             [1, 256, 2, 2]               0\n",
      "          Conv2d-110            [1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-111            [1, 1024, 2, 2]           2,048\n",
      "            ReLU-112            [1, 1024, 2, 2]               0\n",
      "          Conv2d-113             [1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-114             [1, 256, 2, 2]             512\n",
      "            ReLU-115             [1, 256, 2, 2]               0\n",
      "          Conv2d-116             [1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-117             [1, 256, 2, 2]             512\n",
      "            ReLU-118             [1, 256, 2, 2]               0\n",
      "          Conv2d-119            [1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-120            [1, 1024, 2, 2]           2,048\n",
      "            ReLU-121            [1, 1024, 2, 2]               0\n",
      "          Conv2d-122             [1, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-123             [1, 512, 2, 2]           1,024\n",
      "            ReLU-124             [1, 512, 2, 2]               0\n",
      "          Conv2d-125             [1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-126             [1, 512, 1, 1]           1,024\n",
      "            ReLU-127             [1, 512, 1, 1]               0\n",
      "          Conv2d-128            [1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-129            [1, 2048, 1, 1]           4,096\n",
      "            ReLU-130            [1, 2048, 1, 1]               0\n",
      "          Conv2d-131             [1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-132             [1, 512, 1, 1]           1,024\n",
      "            ReLU-133             [1, 512, 1, 1]               0\n",
      "          Conv2d-134             [1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-135             [1, 512, 1, 1]           1,024\n",
      "            ReLU-136             [1, 512, 1, 1]               0\n",
      "          Conv2d-137            [1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-138            [1, 2048, 1, 1]           4,096\n",
      "            ReLU-139            [1, 2048, 1, 1]               0\n",
      "          Conv2d-140             [1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-141             [1, 512, 1, 1]           1,024\n",
      "            ReLU-142             [1, 512, 1, 1]               0\n",
      "          Conv2d-143             [1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-144             [1, 512, 1, 1]           1,024\n",
      "            ReLU-145             [1, 512, 1, 1]               0\n",
      "          Conv2d-146            [1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-147            [1, 2048, 1, 1]           4,096\n",
      "            ReLU-148            [1, 2048, 1, 1]               0\n",
      "          Linear-149                  [1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 22,803,176\n",
      "Trainable params: 22,803,176\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.19\n",
      "Params size (MB): 86.99\n",
      "Estimated Total Size (MB): 90.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50().cuda()\n",
    "try:\n",
    "    model.load_state_dict(torch.load(r\"C:\\Users\\get2b\\Desktop\\Arav\\Neural Networks\\Pytorch implementation\\All Models Saved\\ResNet50.pth\"))\n",
    "except:\n",
    "    print(\"Couldn\\'t find a saved weights of the model... Initializing with Glorot's Initialization of weights and biases...\")\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        model.apply(init_weights) \n",
    "summary(model.cuda(),input_size=(3,32,32),batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=0.0001)\n",
    "lr_scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer,\n",
    "                                           base_lr=1e-5,\n",
    "                                           max_lr=1e-2,\n",
    "                                           step_size_up=5,\n",
    "                                           step_size_down=5,\n",
    "                                           cycle_momentum=False,\n",
    "                                           mode='triangular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i ,data in enumerate(test_loader,0):\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(),labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total+=labels.size(0)\n",
    "            accuracy+=(predicted == labels).sum().item()\n",
    "    \n",
    "    print(\"{}/{}\".format(accuracy,total))\n",
    "    accuracy = float(100 * accuracy / total)\n",
    "    \n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    for ep in range(1,epoch+1):\n",
    "        print(\"Epoch : \",ep)\n",
    "        running_loss=0\n",
    "        for i, data in enumerate(train_loader,0):\n",
    "            inputs,labels = data\n",
    "            inputs,labels = inputs.cuda(),labels.cuda()\n",
    "            optimizer.zero_grad(set_to_none=False)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "            if (i+1)%1000 == 0 :\n",
    "                print(\"[{}/{}]\".format(i+1,len(train_loader.dataset)))\n",
    "                print(\"Loss of the training model is : %.4f\"%(running_loss/1000))\n",
    "                running_loss=0\n",
    "        lr_scheduler.step()\n",
    "        accuracy = test()\n",
    "        print('For epoch', ep,'the test accuracy over the whole test set is %.2f %%' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "[1000/50000]\n",
      "Loss of the training model is : 6.9682\n",
      "89.0/10000.0\n",
      "For epoch 1 the test accuracy over the whole test set is 0.89 %\n",
      "Epoch :  2\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.8328\n",
      "150.0/10000.0\n",
      "For epoch 2 the test accuracy over the whole test set is 1.50 %\n",
      "Epoch :  3\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.4774\n",
      "359.0/10000.0\n",
      "For epoch 3 the test accuracy over the whole test set is 3.59 %\n",
      "Epoch :  4\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.2895\n",
      "435.0/10000.0\n",
      "For epoch 4 the test accuracy over the whole test set is 4.35 %\n",
      "Epoch :  5\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.2003\n",
      "578.0/10000.0\n",
      "For epoch 5 the test accuracy over the whole test set is 5.78 %\n",
      "Epoch :  6\n",
      "[1000/50000]\n",
      "Loss of the training model is : 4.0960\n",
      "662.0/10000.0\n",
      "For epoch 6 the test accuracy over the whole test set is 6.62 %\n",
      "Epoch :  7\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.9908\n",
      "750.0/10000.0\n",
      "For epoch 7 the test accuracy over the whole test set is 7.50 %\n",
      "Epoch :  8\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.9121\n",
      "845.0/10000.0\n",
      "For epoch 8 the test accuracy over the whole test set is 8.45 %\n",
      "Epoch :  9\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8339\n",
      "928.0/10000.0\n",
      "For epoch 9 the test accuracy over the whole test set is 9.28 %\n",
      "Epoch :  10\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.7830\n",
      "997.0/10000.0\n",
      "For epoch 10 the test accuracy over the whole test set is 9.97 %\n",
      "Epoch :  11\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8142\n",
      "995.0/10000.0\n",
      "For epoch 11 the test accuracy over the whole test set is 9.95 %\n",
      "Epoch :  12\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8446\n",
      "962.0/10000.0\n",
      "For epoch 12 the test accuracy over the whole test set is 9.62 %\n",
      "Epoch :  13\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8814\n",
      "896.0/10000.0\n",
      "For epoch 13 the test accuracy over the whole test set is 8.96 %\n",
      "Epoch :  14\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8973\n",
      "943.0/10000.0\n",
      "For epoch 14 the test accuracy over the whole test set is 9.43 %\n",
      "Epoch :  15\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8900\n",
      "952.0/10000.0\n",
      "For epoch 15 the test accuracy over the whole test set is 9.52 %\n",
      "Epoch :  16\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.8129\n",
      "1025.0/10000.0\n",
      "For epoch 16 the test accuracy over the whole test set is 10.25 %\n",
      "Epoch :  17\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.7387\n",
      "1066.0/10000.0\n",
      "For epoch 17 the test accuracy over the whole test set is 10.66 %\n",
      "Epoch :  18\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.6694\n",
      "1184.0/10000.0\n",
      "For epoch 18 the test accuracy over the whole test set is 11.84 %\n",
      "Epoch :  19\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.6086\n",
      "1270.0/10000.0\n",
      "For epoch 19 the test accuracy over the whole test set is 12.70 %\n",
      "Epoch :  20\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.5649\n",
      "1271.0/10000.0\n",
      "For epoch 20 the test accuracy over the whole test set is 12.71 %\n",
      "Epoch :  21\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.5891\n",
      "1209.0/10000.0\n",
      "For epoch 21 the test accuracy over the whole test set is 12.09 %\n",
      "Epoch :  22\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.6337\n",
      "1214.0/10000.0\n",
      "For epoch 22 the test accuracy over the whole test set is 12.14 %\n",
      "Epoch :  23\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.6681\n",
      "1138.0/10000.0\n",
      "For epoch 23 the test accuracy over the whole test set is 11.38 %\n",
      "Epoch :  24\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.6977\n",
      "1035.0/10000.0\n",
      "For epoch 24 the test accuracy over the whole test set is 10.35 %\n",
      "Epoch :  25\n",
      "[1000/50000]\n",
      "Loss of the training model is : 3.7230\n",
      "1091.0/10000.0\n",
      "For epoch 25 the test accuracy over the whole test set is 10.91 %\n"
     ]
    }
   ],
   "source": [
    "train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\get2b\\Desktop\\Arav\\Neural Networks\\Pytorch implementation\\All Models Saved\\ResNet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in losses_of_every_epoch:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a96baaf79f14888d285bcba7f550195ef41d580783ed1a8999b5f8e1380f57b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
